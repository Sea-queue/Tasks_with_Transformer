{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Transformer Model:\n",
    "- Following \n",
    "    - <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\">The Annotated Transformer</a>\n",
    "    - <a href=\"https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853/\" target=\"_blank\">Transformers Explained Visually</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import altair as alt        # for data visualization\n",
    "from torch.optim.lr_scheduler import LambdaLR       \n",
    "# Use LambdaLR to control how the learning rate changes during training — for example, to:\n",
    "# \t•\tWarm up the learning rate for the first few epochs\n",
    "# \t•\tDecay it over time\n",
    "# \t•\tUse a custom schedule\n",
    "\n",
    "PRINT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "- Encoder-decoder structure works the best for translation.  \n",
    "\n",
    "- The encoder maps an \n",
    "    - input sequence of symbol representations $(x_1, ..., x_n)$ to \n",
    "    - a sequence of continuous representations $z = (z_1, ..., z_n)$.\n",
    "- Given z, the decoder\n",
    "    - generates an output sequence $(y_1, ..., y_m)$ of symbols one element a time.\n",
    "    - at each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n",
    "\n",
    "- NOTE:\n",
    "    - The linear layer in MHA is identical to any other linear layer; what makes it “attention” is what you do with those projected Q, K, V matrices afterward:\n",
    "\n",
    "    - **Splitting data across Attention heads:**\n",
    "        - The important thing to understand is that this is a logical split only.  \n",
    "\n",
    "        - The Query, Key, and Value are not physically split into separate matrices, one for each Attention head. A single data matrix is used for the Query, Key, and Value, respectively, with logically separate sections of the matrix for each Attention head. Similarly, there are not separate Linear layers, one for each Attention head. All the Attention heads share the same Linear layer but simply operate on their 'own' logical section of the data matrix: \n",
    "            - reshape: to split the single matrix into chunks\n",
    "            - transpose to have head as the leading axis\n",
    "            - for each head, performand scaled dot-product, softmax weighting.  \n",
    "            - concatenate the score back into one matrix\n",
    "            - The attention weights of single head and multi heads differ, so we genuinely get different attention patterns when splitting into multiple heads rather than do one big head. (Examples down beblow).  \n",
    "            - logically split across multiple heads means the separate sectinos of the Embedding can learn different aspects of the meanings of each word. \n",
    "            - for the $\\mathrm{Attention}(Q,K,V) = \\mathrm{softmax}\\!\\Bigl(\\tfrac{QK^T}{\\sqrt{d_k}}\\Bigr)\\,V$, the weights (the softmax of $QK^T$) changes \n",
    "            - This allows the transformer to capture richer interpretations of the squence. (for the word 'men': it has genderness(male, female) and cardinality(singular vs plural))\n",
    "\n",
    "        - Think of it as ‘stacking together’ the separate layer weights for each head.  \n",
    "\n",
    "        - The computations for all Heads can be therefore be achieved via a single matrix operation rather than requiring N separate operations.   \n",
    "\n",
    "        - This makes the computations more efficient and keeps the model simple because fewer Linear layers are required, while still achieving the power of the independent Attention heads.   \n",
    "\n",
    "    - during training, we provide both encoder and decoder inputs, and the model gets all the predicted outputs at once using teacher-forcing.  \n",
    "\n",
    "    - during inference, the encoder only calculate once because the encoder input never changes. And decoder start with <start> symbol, accumulates the decoder output and feedback to the decoder for the next loop.  \n",
    "\n",
    "    - The size of encoder's input and output(input of encoder-decoder cross-attention) remain the same. [Batch, Sequence1, Dimension] --> [Batch, Sequence1, Dimension]  \n",
    "\n",
    "    - The size of Decoder's input and first self-attention layer output remain the same. [Batch, Sequence2, Dimension] --> [Batch, Sequence2, Dimension]\n",
    "\n",
    "    - In the encoder–decoder “cross‐attention” sub-layer of the Transformer decoder, the three inputs are:\n",
    "\t    - Query $Q\\in\\mathbb{R}^{B\\times T_\\text{dec}\\times D}$ – your decoder’s own hidden states (after self-attention + residual).\n",
    "\t    - Key $K\\in\\mathbb{R}^{B\\times T_\\text{enc}\\times D}$ – the encoder’s final hidden states.\n",
    "\t    - Value  $V\\in\\mathbb{R}^{B\\times T_\\text{enc}\\times D}$ – again the encoder’s final hidden states.\n",
    "\t    - B = batch size\n",
    "\t    - $T_\\text{dec}$ = decoder sequence length\n",
    "\t    - $T_\\text{enc}$ = encoder sequence length\n",
    "\t    - D = model dimension  \n",
    "\n",
    "    - The final output of Decoder has [Batch, Sequence2, Dimension], same as the decoder's input.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    '''A standard Encoder-Decoder architecture. Base for this and many other models.'''\n",
    "     \n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        # python 3\n",
    "        # super().__init__()\n",
    "        # python 2\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    # what are these masks for why do we need two masks?\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # src_mask (on encoder's input) is to make sure the inputs have the same length.\n",
    "        # tgt_mask (on decoder's input) is to prevent the model to cheat by seeing the results (next word it's supposed to predict), zigzag zero padding.\n",
    "        # NOTE: here it's calling the self-defined encode and decode functions, and switch the order of the parameters.\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        if PRINT:\n",
    "            print(f'in EncoderDecoder before embedding(Encoder): src: {src.shape}; src_mask: {src_mask.shape}')\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        if PRINT:\n",
    "            print(f'in EncoderDecoder before embedding(Decoder): tgt: {tgt.shape}; tgt_mask: {tgt_mask.shape}')\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    ''' Standard linear + softmax generation step '''\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    # Produce N identical layers\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    # diagonal=0: includes the main diagonal.\n",
    "\t# diagonal=1: starts one step above the main diagonal.\n",
    "\t# diagonal=-1: starts one step below the main diagonal.\n",
    "    # torch.triu(..., diagonal=1) returns the upper triangular part\n",
    "    # [[[0, 1, 1, 1, 1],\n",
    "    #   [0, 0, 1, 1, 1],\n",
    "    #   [0, 0, 0, 1, 1],\n",
    "    #   [0, 0, 0, 0, 1],\n",
    "    #   [0, 0, 0, 0, 0]]]\n",
    "    # .type(torch.uint8) converts it to 0 (for allowed) and 1 (for masked)\n",
    "    # Later when apply mask:\n",
    "    # scores = scores.masked_fill(mask == 0, 0)   # replaces elements of the tensor where the condition is True with the given value 0.\n",
    "    # scores = scores.masked_fill(mask == 1, -inf)  # replaces elements of the tensor where the condition is True with the given value -inf.\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "\n",
    "    # Return a boolean mask tensor where each element is True if the corresponding element in subsequent_mask is 0, and False if it’s 1.\n",
    "    # tensor([[[0, 1, 1],\n",
    "    #          [0, 0, 1],\n",
    "    #          [0, 0, 0]]], dtype=torch.uint8)\n",
    "    # would return\n",
    "    # tensor([[[ True, False, False],\n",
    "    #          [ True,  True, False],\n",
    "    #          [ True,  True,  True]]])\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h:int, d_model:int, dropout = 0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        # NOTE: 4 linear layers are Q,K,V, and OUTPUT projection\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query:torch.Tensor, key:torch.Tensor, value:torch.Tensor, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        if PRINT:\n",
    "            print('In MHA:')\n",
    "            for i, layer in enumerate(self.linears):\n",
    "                print(f\"Layer {i}: in_features = {layer.in_features}, out_features = {layer.out_features}\")\n",
    "                print(f'layer {i} weights: ', layer.weight)\n",
    "        \n",
    "        if PRINT:\n",
    "            print(f'QKV shape BEFORE split: {query.shape}, {key.shape}, {value.shape}')\n",
    "        # MEAT AND POTATOES!!! \n",
    "        # self.linears is a list of four nn.Linear(d_model, d_model) layers; here we only consume the first three in the comprehension (for Q, K, V)\n",
    "        # zip pairs them up as\n",
    "        # 1. (l = W^Q, x = query)\n",
    "        # 2. (l = W^K, x = key)\n",
    "        # 3. (l = W^V, x = value)\n",
    "        # l(x): Applies the linear projection\n",
    "        # .view reshapes the qkv from (batch_size, seq_len, d_model) --> (batch_size, num_heads, seq_len, d_k):\n",
    "        # nbatches = batch_size.\n",
    "        # -1 tells PyTorch to infer that dimension — it will become seq_len.\n",
    "        # self.h = number of heads.\n",
    "        # self.d_k = d_model // num_heads.\n",
    "        # .transpose(1, 2) Swaps dimensions 1 and 2 so head is a leading axis: \n",
    "        # from (batch_size, seq_len, num_heads, d_k) --> (batch_size, num_heads, seq_len, d_k)\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        if PRINT:\n",
    "            print(f'QKV shape AFTER split: {query.shape}, {key.shape}, {value.shape}')\n",
    "            print(f'query: {query}')\n",
    "            print(f'key: {key}')\n",
    "            print(f'value: {value}')\n",
    "\n",
    "        # .view() + .transpose() carves on matrix into h separate “heads” of size d_k each\n",
    "        # each head’s chunk is used to compute its own attention scores\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = (x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        mha_return_val = self.linears[-1](x)\n",
    "        if PRINT:\n",
    "            print('MHA return val shape:', mha_return_val.shape)\n",
    "            print(f'MHA return val:', mha_return_val)\n",
    "        return mha_return_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the Multi-Head Attention:\n",
    "\n",
    "\n",
    "- Say the seq_len = 3, and head_dim = 3  \n",
    "\n",
    "- $\\begin{bmatrix} 0.7 & 0.2 & 0.1 \\\\ 0.8 & 0.1 & 0.1 \\\\ 0.3 & 0.3 & 0.4 \\end{bmatrix}$\n",
    "\n",
    "- Each row of this matrix says: “How much attention this token pays to all tokens in the sequence (including itself).”  \n",
    "- Row 0 → Token 0 pays 70% attention to itself, 20% to token 1, 10% to token 2\n",
    "- Row 1 → Token 1 pays 80% attention to itself, etc.\n",
    "- Each row sums to 1 — like a probability distribution over the other tokens.\n",
    "- when perform the matrix multiplication, the second token has 0.8 on first token, and this weight will multiply with the first token's vector from the Val. (the characteristic of matrix multiplication) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-head raw scores:\n",
      " tensor([[[ 0., 10.],\n",
      "         [ 4., 30.]]])\n",
      "Single-head softmax:\n",
      " tensor([[[4.5398e-05, 9.9995e-01],\n",
      "         [5.1091e-12, 1.0000e+00]]])\n",
      "\n",
      "Head 0 raw scores:\n",
      "tensor([[ 2.8284,  7.0711],\n",
      "        [11.3137, 26.8701]])\n",
      "Head 0 softmax:\n",
      "tensor([[1.4166e-02, 9.8583e-01],\n",
      "        [1.7537e-07, 1.0000e+00]])\n",
      "\n",
      "Head 1 raw scores:\n",
      "tensor([[-2.8284,  7.0711],\n",
      "        [-5.6569, 15.5563]])\n",
      "Head 1 softmax:\n",
      "tensor([[5.0197e-05, 9.9995e-01],\n",
      "        [6.1266e-10, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Define Q, K\n",
    "Q = torch.tensor([[[1.,2.,3.,4.],\n",
    "                   [5.,6.,7.,8.]]])  # (1, 2, 4)\n",
    "K = torch.tensor([[[2.,1.,0.,-1.],\n",
    "                   [4.,3.,2.,1.]]])  # (1, 2, 4)\n",
    "\n",
    "# 1) Single-head (d_model=4)\n",
    "scores_single = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(4)\n",
    "attn_single  = torch.softmax(scores_single, dim=-1)\n",
    "\n",
    "print(\"Single-head raw scores:\\n\", scores_single)\n",
    "print(\"Single-head softmax:\\n\", attn_single)\n",
    "\n",
    "# 2) Multi-head (h=2, d_k=2)\n",
    "d_model, h = 4, 2\n",
    "d_k = d_model // h\n",
    "\n",
    "# reshape & transpose into heads\n",
    "Qh = Q.view(1, 2, h, d_k).transpose(1,2)  # (1, 2, 2, 2)->(1, h, 2, d_k)\n",
    "Kh = K.view(1, 2, h, d_k).transpose(1,2)\n",
    "\n",
    "scores_heads = torch.matmul(Qh, Kh.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "attn_heads  = torch.softmax(scores_heads, dim=-1)\n",
    "\n",
    "for i in range(h):\n",
    "    print(f\"\\nHead {i} raw scores:\\n{scores_heads[0,i]}\")\n",
    "    print(f\"Head {i} softmax:\\n{attn_heads[0,i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Q1: \n",
      " [[ 0.49671415 -0.1382643 ]\n",
      " [-0.23415337 -0.23413696]\n",
      " [-0.46947439  0.54256004]\n",
      " [ 0.24196227 -1.91328024]]\n",
      "W_K1: \n",
      " [[-1.01283112  0.31424733]\n",
      " [ 1.46564877 -0.2257763 ]\n",
      " [-0.54438272  0.11092259]\n",
      " [-0.60063869 -0.29169375]]\n",
      "W_V1: \n",
      " [[-0.01349722 -1.05771093]\n",
      " [ 0.2088636  -1.95967012]\n",
      " [ 0.73846658  0.17136828]\n",
      " [-1.47852199 -0.71984421]]\n",
      "W_Q2: \n",
      " [[ 0.64768854  1.52302986]\n",
      " [ 1.57921282  0.76743473]\n",
      " [-0.46341769 -0.46572975]\n",
      " [-1.72491783 -0.56228753]]\n",
      "W_K2: \n",
      " [[-0.90802408 -1.4123037 ]\n",
      " [ 0.0675282  -1.42474819]\n",
      " [-1.15099358  0.37569802]\n",
      " [-0.60170661  1.85227818]]\n",
      "W_V2: \n",
      " [[ 0.82254491 -1.22084365]\n",
      " [-1.32818605  0.19686124]\n",
      " [-0.11564828 -0.3011037 ]\n",
      " [-0.46063877  1.05712223]]\n",
      "Single-Head output:\n",
      " tensor([[[0.0699, 0.0171, 0.0416, 0.8715],\n",
      "         [0.0699, 0.0171, 0.0416, 0.8715]]]) \n",
      "\n",
      "Multi-Head output:\n",
      " tensor([[[0.8106, 0.1894],\n",
      "         [0.8074, 0.1926]],\n",
      "\n",
      "        [[0.0455, 0.9545],\n",
      "         [0.0455, 0.9545]]]) \n",
      "\n",
      "Head 1 output:\n",
      " tensor([[[0.8106, 0.1894],\n",
      "         [0.8074, 0.1926]]]) \n",
      "\n",
      "Head 2 output:\n",
      " tensor([[[0.0455, 0.9545],\n",
      "         [0.0455, 0.9545]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# ========== 1) Clear Q, K, V ==========\n",
    "Q = np.array([[\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]])  # shape (1,2,4)\n",
    "K = np.array([[\n",
    "    [2, 1, 0, -1],\n",
    "    [4, 3, 2, 1]\n",
    "]])  # shape (1,2,4)\n",
    "V = np.array([[\n",
    "    [1, 0, 1, 0],\n",
    "    [0, 1, 0, 1]\n",
    "]])  # shape (1,2,4)\n",
    "\n",
    "# ========== 2) Head dimensions ==========\n",
    "d_model = 4\n",
    "d_k     = 2\n",
    "d_v     = 2\n",
    "h = 2\n",
    "\n",
    "# ========== 3) Helpers ==========\n",
    "def softmax(x, axis=-1):\n",
    "    e = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v):\n",
    "    scores  = np.matmul(q, k.transpose(0,2,1)) / np.sqrt(q.shape[-1])\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    return np.matmul(weights, v)\n",
    "\n",
    "# ========== 4) Single-head projection weights ==========\n",
    "W_Q0 = np.random.randn(d_model, h*d_k)\n",
    "W_K0 = np.random.randn(d_model, h*d_k)\n",
    "W_V0 = np.random.randn(d_model, h*d_v)\n",
    "\n",
    "# Project Q, K, V and attend once\n",
    "Q0 = Q @ W_Q0\n",
    "K0 = K @ W_K0\n",
    "V0 = V @ W_V0\n",
    "out_single = scaled_dot_product_attention(Q0, K0, V0)\n",
    "out_single_t  = torch.from_numpy(out_single).float() \n",
    "attn_single  = torch.softmax(out_single_t, dim=-1)\n",
    "\n",
    "# ========== 5) Two additional heads ==========\n",
    "Qh = Q0.reshape(2, h, d_k).transpose(1, 0, 2)  # (2, 2, 2)->(h, 2, d_k)\n",
    "Kh = K0.reshape(2, h, d_k).transpose(1, 0, 2)\n",
    "Vh = V0.reshape(2, h, d_k).transpose(1, 0, 2)\n",
    "head_out = scaled_dot_product_attention(Qh, Kh, Vh)\n",
    "head_out_t  = torch.from_numpy(head_out).float() \n",
    "attn_head  = torch.softmax(head_out_t, dim=-1)\n",
    "\n",
    "# Head 1\n",
    "W_Q1 = W_Q0[:, :d_k]\n",
    "print(\"W_Q1: \\n\", W_Q1)\n",
    "W_K1 = W_K0[:, :d_k]\n",
    "print(\"W_K1: \\n\", W_K1)\n",
    "W_V1 = W_V0[:, :d_v]\n",
    "print(\"W_V1: \\n\", W_V1)\n",
    "Q1, K1, V1 = Q @ W_Q1, K @ W_K1, V @ W_V1\n",
    "out_head1  = scaled_dot_product_attention(Q1, K1, V1)\n",
    "out_head1_t  = torch.from_numpy(out_head1).float() \n",
    "attn_head1  = torch.softmax(out_head1_t, dim=-1)\n",
    "\n",
    "# Head 2\n",
    "W_Q2 = W_Q0[:, d_k:]\n",
    "print(\"W_Q2: \\n\", W_Q2)\n",
    "W_K2 = W_K0[:, d_k:]\n",
    "print(\"W_K2: \\n\", W_K2)\n",
    "W_V2 = W_V0[:, d_v:]\n",
    "print(\"W_V2: \\n\", W_V2)\n",
    "Q2, K2, V2 = Q @ W_Q2, K @ W_K2, V @ W_V2\n",
    "out_head2  = scaled_dot_product_attention(Q2, K2, V2)\n",
    "out_head2_t  = torch.from_numpy(out_head2).float() \n",
    "attn_head2  = torch.softmax(out_head2_t, dim=-1)\n",
    "\n",
    "# ========== 6) Print ==========\n",
    "print(\"Single-Head output:\\n\", attn_single, \"\\n\")\n",
    "print(\"Multi-Head output:\\n\", attn_head, \"\\n\")\n",
    "print(\"Head 1 output:\\n\", attn_head1, \"\\n\")\n",
    "print(\"Head 2 output:\\n\", attn_head2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_t: \n",
      " tensor([[[1., 2., 3., 4.],\n",
      "         [5., 6., 7., 8.]]])\n",
      "K_t: \n",
      " tensor([[[ 2.,  1.,  0., -1.],\n",
      "         [ 4.,  3.,  2.,  1.]]])\n",
      "V_t: \n",
      " tensor([[[1., 0., 1., 0.],\n",
      "         [0., 1., 0., 1.]]])\n",
      "In MHA:\n",
      "Layer 0: in_features = 4, out_features = 4\n",
      "layer 0 weights:  Parameter containing:\n",
      "tensor([[ 0.1958, -0.3091,  0.0778,  0.1922],\n",
      "        [ 0.3054,  0.3775,  0.1262,  0.3985],\n",
      "        [-0.4576,  0.1862, -0.0031,  0.2265],\n",
      "        [-0.2400,  0.4219,  0.2924,  0.0081]], requires_grad=True)\n",
      "Layer 1: in_features = 4, out_features = 4\n",
      "layer 1 weights:  Parameter containing:\n",
      "tensor([[ 0.1958, -0.3091,  0.0778,  0.1922],\n",
      "        [ 0.3054,  0.3775,  0.1262,  0.3985],\n",
      "        [-0.4576,  0.1862, -0.0031,  0.2265],\n",
      "        [-0.2400,  0.4219,  0.2924,  0.0081]], requires_grad=True)\n",
      "Layer 2: in_features = 4, out_features = 4\n",
      "layer 2 weights:  Parameter containing:\n",
      "tensor([[ 0.1958, -0.3091,  0.0778,  0.1922],\n",
      "        [ 0.3054,  0.3775,  0.1262,  0.3985],\n",
      "        [-0.4576,  0.1862, -0.0031,  0.2265],\n",
      "        [-0.2400,  0.4219,  0.2924,  0.0081]], requires_grad=True)\n",
      "Layer 3: in_features = 4, out_features = 4\n",
      "layer 3 weights:  Parameter containing:\n",
      "tensor([[ 0.1958, -0.3091,  0.0778,  0.1922],\n",
      "        [ 0.3054,  0.3775,  0.1262,  0.3985],\n",
      "        [-0.4576,  0.1862, -0.0031,  0.2265],\n",
      "        [-0.2400,  0.4219,  0.2924,  0.0081]], requires_grad=True)\n",
      "QKV shape BEFORE split: torch.Size([1, 2, 4]), torch.Size([1, 2, 4]), torch.Size([1, 2, 4])\n",
      "QKV shape AFTER split: torch.Size([1, 2, 2, 2]), torch.Size([1, 2, 2, 2]), torch.Size([1, 2, 2, 2])\n",
      "query: tensor([[[[0.1356, 2.8647],\n",
      "          [0.7628, 7.6951]],\n",
      "\n",
      "         [[0.3836, 1.5896],\n",
      "          [0.1915, 3.5187]]]], grad_fn=<TransposeBackward0>)\n",
      "key: tensor([[[[-0.5541,  0.4217],\n",
      "          [-0.2405,  2.8369]],\n",
      "\n",
      "         [[-1.3832,  0.0102],\n",
      "          [-1.4793,  0.9748]]]], grad_fn=<TransposeBackward0>)\n",
      "value: tensor([[[[-0.1708,  0.2635],\n",
      "          [-0.5613,  0.6078]],\n",
      "\n",
      "         [[-0.8884,  0.1288],\n",
      "          [-0.0151,  0.5064]]]], grad_fn=<TransposeBackward0>)\n",
      "MHA return val shape: torch.Size([1, 2, 4])\n",
      "MHA return val: tensor([[[-0.7072,  0.0436,  0.0852,  0.4348],\n",
      "         [-0.3507,  0.0295, -0.3080,  0.0519]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# multi head attention example\n",
    "# head=2; d_model=4\n",
    "Q_t = torch.from_numpy(Q).float()\n",
    "print(\"Q_t: \\n\", Q_t)\n",
    "K_t = torch.from_numpy(K).float()\n",
    "print(\"K_t: \\n\", K_t)\n",
    "V_t = torch.from_numpy(V).float()\n",
    "print(\"V_t: \\n\", V_t)\n",
    "mha = MultiHeadedAttention(2, 4)\n",
    "mha_output = mha(Q_t, K_t, V_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Construct a layernorm module (See citation for details).'''\n",
    "    # TODO: what is the purpose of eps?\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization:\n",
    "\n",
    "- Any technique that re-scales intermediate activations(outputs of network’s neurons) so they have (typically) zero mean and unit variance.\n",
    "    - zero mean: \n",
    "        - mean(average): $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$\n",
    "        - shift data so that the average $\\mu$ becomes 0.\n",
    "    - unit variance: \n",
    "        - The variance measures how spread-out the values are around the mean: $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2$\n",
    "        - The standard deviation is $\\sigma = \\sqrt{\\sigma^2}$\n",
    "        - “Unit variance” means we scale our data so that $\\sigma^2 = 1$ (equivalently, $\\sigma = 1$):\n",
    "            - on average, the squared deviations of values from their mean is exactly one.  \n",
    "            - no single dimension can dominate purely by having larger numeric scale.  \n",
    "\t        - It makes gradient updates more uniform across parameters, which stabilizes and speeds up training.  \n",
    "    \n",
    "    - Standardization:\n",
    "        - To transform an arbitrary dataset (or activations) into zero mean and unit variance, we compute $\\hat x_i = \\frac{x_i - \\mu}{\\sigma}$\n",
    "        - After this operation, the new values $\\{\\hat x_i\\}$ satisfy $\\frac1n\\sum_i \\hat x_i = 0$ and $\\frac1n\\sum_i (\\hat x_i - 0)^2 = 1$\n",
    "    - Benefits: \n",
    "        - Zero mean recenters your data around 0, so positive and negative signals balance out.\n",
    "        - Unit variance ensures no feature “blows up” the scale—every dimension contributes similarly.\n",
    "        - Stabilizing the learning dynamics by keeping activations in a predictable range\n",
    "        - Smoothing the loss surface, which often speeds up convergence and allows for larger learning rates\n",
    "\n",
    "- Batch Norm:\n",
    "\n",
    "- Layer Norm:\n",
    "\n",
    "- Post Norm:\n",
    "\n",
    "- Pre Norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # NOTE: the sublayer is a lambda wrapper on the multi-head attention from encoder & decoder\n",
    "        # it performs Pre-Layer-Norm because it's more stable to train very deep models, pos-norm suffer from gradient vanishing.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn:MultiHeadedAttention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer:EncoderLayer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The encoder is composed of a stack of N identical layers.\n",
    "        # repeat the entire Encoder layer (multi-head attention, feedforward, layer_norm...) N times.\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        if PRINT:\n",
    "            print(f'in Encoder after embedding: src:{x.shape}; src_mask: {mask.shape}')\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder-decoder attention:\n",
    "- the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.\n",
    "\n",
    "### encoder attention & decoder attention:\n",
    "- all of the keys, values and queries come from the same place (the output of the previous layer)\n",
    "- Each position can attend to all positions in the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn:MultiHeadedAttention, src_attn:MultiHeadedAttention, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer:DecoderLayer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        # repeat the entire Decoder layer (2 multi-head attention, feedforward, layer_norm...) N times.\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        if PRINT:\n",
    "            print(f'in Decoder after embedding: tgt: {x.shape}; encoder_output:{memory.shape}; encoder_output_mask: {src_mask.shape}; tgt_mask:{tgt_mask.shape}')\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if PRINT:\n",
    "            print(f'in FF: input shape: {x.shape}')\n",
    "        ff_val = self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "        if PRINT:\n",
    "            print(f'in FF: output shape: {x.shape}')\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does Embedding layer do:\n",
    "\n",
    "- This embedding layer maps each token index to a d_model-dimensional vector.\n",
    "- self.lut(x) Looks up embeddings for the token indices x. Then, scales the embeddings by a constant factor $\\sqrt{d_{model}}$ \n",
    "- This is a normalization trick to help with training stability. The dot-product attention has a scaling factor $\\frac{1}{\\sqrt{d_k}}$ so scaling the input embeddings helps balance the magnitudes.  \n",
    "\n",
    "- Without this scaling, the softmax in attention could produce very small gradients, especially at the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # This is the look up table, retrieving vectors using token IDs.\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Positional Encoding:\n",
    "- Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model= 512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    # In PyTorch, every neural network(subclasses of nn.Module) has two of the most important “modes”: training (.train()) and evaluation (.eval()).\n",
    "    test_model.eval()\n",
    "    # This constructs a tensor of 64-bit integers\n",
    "    # has shape (1, 10):\n",
    "\t# 1 is the batch size (you have one sequence).\n",
    "\t# 10 is the sequence length (ten tokens, with IDs 1 through 10).\n",
    "    # feed this through an nn.Embedding to turn each integer into a vector\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    print(f'src: {src}')\n",
    "    # create 1s of shape (1, 1, 10), mask value of 1 means to keep.\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "    print(f'src_mask: {src_mask}')\n",
    "\n",
    "    # the mask tells the attention mechanism which source positions to consider. All-ones means “fully attend to every token.”\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    print(f'memory: {memory}')\n",
    "    # torch.zeros(1, 1): Creates a new tensor of shape (1, 1) filled with 0.0; By default this is a floating-point tensor (dtype=torch.float32) on the CPU.\n",
    "    # type_as(src.data):\t\n",
    "    # •\tTakes whatever tensor you pass in—here src.data—and queries its dtype and device.\n",
    "\t# •\tCasts (and, if needed, moves) the zeros-tensor so it matches that type and device.\n",
    "\t# •\tIf src was a LongTensor on the GPU, ys becomes a LongTensor on the GPU; if src was an FloatTensor on CPU, ys ends up the same, and so on.\n",
    "    ys = torch.zeros(1, 1).type_as(src.data)\n",
    "    print(f'ground truth: {ys}')\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        print(f'loop {i} out: {out.shape}')\n",
    "        # out is decoder’s hidden‐state tensor of shape (batch_size, seq_len, d_model).\n",
    "        # out[:, -1] grabs the last time-step for each item in the batch, giving a tensor of shape (batch_size, d_model).\n",
    "        # test_model.generator is typically a nn.Linear(d_model, vocab_size) (often followed by log-softmax), so prob ends up as (batch_size, vocab_size) containing the (log-)probabilities of every possible next token.\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        print(f'loop{i} prob: {prob}')\n",
    "        \n",
    "        # torch.max(..., dim=1) returns two things:\n",
    "        # •\tThe maximum values (which throw away into _), and\n",
    "        # •\tThe indices of those maxima along dim=1 (the vocabulary dimension).\n",
    "        # So next_word is a tensor of shape (batch_size,) containing the predicted token IDs.\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        print(f'loop {i} next word: {next_word}')\n",
    "        # .data[0] grabs the first element of the tensor (and gives you a raw Python scalar or zero-dim tensor).\n",
    "\t    # Note: in modern PyTorch you’d usually write next_word = next_word.item().\n",
    "        next_word = next_word.data[0]\n",
    "        print(f'loop {i} next word: {next_word}')\n",
    "        ys = torch.cat([ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        print(f'loop {i} ground truth: {ys}')\n",
    "    \n",
    "    print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0751, -1.3743, -0.6627, -1.3150, -0.1900],\n",
      "         [ 0.6102,  0.5823,  0.9035,  0.3814,  0.5704],\n",
      "         [-0.3351,  0.3250,  0.0986, -1.7480,  1.2289]],\n",
      "\n",
      "        [[ 0.2573,  1.1233, -0.7100, -0.6179,  0.6549],\n",
      "         [-0.0216, -0.2573,  0.0491, -0.1773, -1.9397],\n",
      "         [-0.0931,  0.4979,  0.6578,  0.3862, -0.7730]]])\n",
      "torch.Size([2, 5])\n",
      "tensor([[-0.3351,  0.3250,  0.0986, -1.7480,  1.2289],\n",
      "        [-0.0931,  0.4979,  0.6578,  0.3862, -0.7730]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 5)   # say batch_size=4, seq_len=10, d_model=512\n",
    "y = x[:, -1]                  # integer “-1”\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "src_mask: tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
      "memory: tensor([[[ 0.7866,  1.2446, -0.5330,  ..., -1.0366,  1.2295, -0.7082],\n",
      "         [ 1.4092,  1.7337, -0.0820,  ...,  0.8748, -0.1855, -0.0618],\n",
      "         [ 2.3700,  0.7851,  0.8545,  ..., -0.5111, -0.1885, -0.4453],\n",
      "         ...,\n",
      "         [ 1.0476,  0.7221, -0.1984,  ..., -0.8055, -0.9070, -0.1119],\n",
      "         [ 1.5334,  1.2891, -1.2536,  ..., -0.3821, -0.7405,  1.7064],\n",
      "         [ 1.7038,  0.5228,  1.3222,  ..., -0.2475,  0.2649,  0.6735]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ground truth: tensor([[0]])\n",
      "loop 0 out: torch.Size([1, 1, 512])\n",
      "loop0 prob: tensor([[-3.5683, -3.5945, -3.4997, -3.4052, -1.9056, -1.9189, -1.6448, -2.0978,\n",
      "         -1.5085, -3.1557, -5.1526]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 0 next word: tensor([8])\n",
      "loop 0 next word: 8\n",
      "loop 0 ground truth: tensor([[0, 8]])\n",
      "loop 1 out: torch.Size([1, 2, 512])\n",
      "loop1 prob: tensor([[-4.1903, -4.1803, -2.2895, -3.9797, -2.9981, -3.2769, -3.3059, -3.7760,\n",
      "         -1.4499, -1.4029, -1.5056]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 1 next word: tensor([9])\n",
      "loop 1 next word: 9\n",
      "loop 1 ground truth: tensor([[0, 8, 9]])\n",
      "loop 2 out: torch.Size([1, 3, 512])\n",
      "loop2 prob: tensor([[-3.4586, -6.3508, -4.4762, -5.5950, -4.0330, -0.1846, -5.5652, -3.6741,\n",
      "         -3.4099, -3.4633, -4.7190]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 2 next word: tensor([5])\n",
      "loop 2 next word: 5\n",
      "loop 2 ground truth: tensor([[0, 8, 9, 5]])\n",
      "loop 3 out: torch.Size([1, 4, 512])\n",
      "loop3 prob: tensor([[-5.7401, -5.6973, -2.5075, -5.5270, -3.9630, -0.6353, -5.4191, -7.2506,\n",
      "         -3.2202, -1.2313, -3.8075]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 3 next word: tensor([5])\n",
      "loop 3 next word: 5\n",
      "loop 3 ground truth: tensor([[0, 8, 9, 5, 5]])\n",
      "loop 4 out: torch.Size([1, 5, 512])\n",
      "loop4 prob: tensor([[-5.6750, -5.3418, -2.4904, -5.5079, -3.8634, -0.4924, -5.1123, -7.0893,\n",
      "         -2.8051, -1.6980, -3.8037]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 4 next word: tensor([5])\n",
      "loop 4 next word: 5\n",
      "loop 4 ground truth: tensor([[0, 8, 9, 5, 5, 5]])\n",
      "loop 5 out: torch.Size([1, 6, 512])\n",
      "loop5 prob: tensor([[-5.5833, -4.9554, -2.3532, -5.3686, -3.7686, -0.4874, -4.6604, -7.0221,\n",
      "         -2.4402, -2.0356, -3.7279]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 5 next word: tensor([5])\n",
      "loop 5 next word: 5\n",
      "loop 5 ground truth: tensor([[0, 8, 9, 5, 5, 5, 5]])\n",
      "loop 6 out: torch.Size([1, 7, 512])\n",
      "loop6 prob: tensor([[-5.4665, -4.6394, -2.1448, -5.1171, -3.6245, -0.5898, -4.1472, -7.0826,\n",
      "         -2.1264, -2.1281, -3.6146]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 6 next word: tensor([5])\n",
      "loop 6 next word: 5\n",
      "loop 6 ground truth: tensor([[0, 8, 9, 5, 5, 5, 5, 5]])\n",
      "loop 7 out: torch.Size([1, 8, 512])\n",
      "loop7 prob: tensor([[-5.4074, -4.4245, -1.9728, -4.8206, -3.4004, -0.7483, -3.7759, -7.2163,\n",
      "         -1.8999, -2.0578, -3.5438]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 7 next word: tensor([5])\n",
      "loop 7 next word: 5\n",
      "loop 7 ground truth: tensor([[0, 8, 9, 5, 5, 5, 5, 5, 5]])\n",
      "loop 8 out: torch.Size([1, 9, 512])\n",
      "loop8 prob: tensor([[-5.4456, -4.3379, -1.9153, -4.5389, -3.2098, -0.8421, -3.7181, -7.3448,\n",
      "         -1.8037, -1.9965, -3.5743]], grad_fn=<LogSoftmaxBackward0>)\n",
      "loop 8 next word: tensor([5])\n",
      "loop 8 next word: 5\n",
      "loop 8 ground truth: tensor([[0, 8, 9, 5, 5, 5, 5, 5, 5, 5]])\n",
      "Example Untrained Model Prediction: tensor([[0, 8, 9, 5, 5, 5, 5, 5, 5, 5]])\n"
     ]
    }
   ],
   "source": [
    "for _ in range (1):\n",
    "    inference_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tools to train a standard encoder-decoder model\n",
    "- Batch object: holds src and target sentences for training, construct masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, tgt=None, pad=2, device='cpu'):\n",
    "        self.src = src\n",
    "        # Build a mask over src to hide padding tokens\n",
    "        # src = torch.LongTensor([[5, 7, 2, 3],\n",
    "        #                         [4, 2, 2, 1]])\n",
    "        # pad = 2\n",
    "        # mask = (src != pad)\n",
    "        # mask is now:\n",
    "        # tensor([[ True,  True, False,  True],\n",
    "        #         [ True, False, False,  True]])\n",
    "        # .unsqueeze(-2) adds a dimension so src_mask is shape (batch_size, 1, src_len).\n",
    "        self.src_mask = (src != pad).unsqueeze(-2).to(device)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1].to(device)\n",
    "            self.tgt_y = tgt[:, 1:].to(device)\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad).to(device)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum().to(device)\n",
    "            \n",
    "    # A static function belongs to a class rather than an instance of the class.\n",
    "\t# class MyClass:\n",
    "    #     @staticmethod\n",
    "    #     def greet(name):    # No self or cls parameter\n",
    "    #         return f\"Hello, {name}!\"\n",
    "    # # Usage:\n",
    "    # MyClass.greet(\"Alice\")  # No need to create an instance\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generic training and scoring function to keep track of loss. \n",
    "- We pass in a generic loss compute function that also handles parameter updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "    \n",
    "    # This is not the same as using __init__ or @dataclass. This defines default class variables, not assigning these to each instance (Values can NOT be modified per object).\n",
    "    step: int = 0           # Steps in the current epoch\n",
    "    accum_step: int = 0     # Number of gradient accumulation steps\n",
    "    samples: int = 0        # total number of examples used\n",
    "    tokens: int = 0         # total number of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute, optimizer, scheduler, mode=\"train\", accum_iter=1,train_state=TrainState()):\n",
    "    \n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_learning_schedule():\n",
    "    opts = [\n",
    "        [512, 1, 4000],  # example 1\n",
    "        [512, 1, 8000],  # example 2\n",
    "        [256, 1, 4000],  # example 3\n",
    "    ]\n",
    "\n",
    "    dummy_model = torch.nn.Linear(1, 1)\n",
    "    learning_rates = []\n",
    "\n",
    "    # we have 3 examples in opts list.\n",
    "    for idx, example in enumerate(opts):\n",
    "        # run 20000 epoch for each example\n",
    "        # Adaptive Moment Estimation(Adam): an algorithm used to update the parameters (weights) of a neural network during training\n",
    "        optimizer = torch.optim.Adam(dummy_model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9) \n",
    "        # Use LambdaLR to control how the learning rate changes during training — for example, to:\n",
    "        # •\tWarm up the learning rate for the first few epochs\n",
    "        # •\tDecay it over time\n",
    "        # •\tUse a custom schedule\n",
    "        lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: rate(step, *example))\n",
    "        tmp = []\n",
    "        # take 20K dummy training steps, save the learning rate at each step\n",
    "        for step in range(20000):\n",
    "            tmp.append(optimizer.param_groups[0][\"lr\"])\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "        learning_rates.append(tmp)\n",
    "\n",
    "    learning_rates = torch.tensor(learning_rates)\n",
    "\n",
    "    # Enable altair to handle more than 5000 rows\n",
    "    alt.data_transformers.disable_max_rows()\n",
    "\n",
    "    opts_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Learning Rate\": learning_rates[warmup_idx, :],\n",
    "                    \"model_size:warmup\": [\"512:4000\", \"512:8000\", \"256:4000\"][\n",
    "                        warmup_idx\n",
    "                    ],\n",
    "                    \"step\": range(20000),\n",
    "                }\n",
    "            )\n",
    "            for warmup_idx in [0, 1, 2]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(opts_data)\n",
    "        .mark_line()\n",
    "        .properties(width=600)\n",
    "        .encode(x=\"step\", y=\"Learning Rate\", color=\"model_size:warmup:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "# example_learning_schedule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    '''\n",
    "    Implement label smoothing:\n",
    "    Normally in classification, we use one-hot vectors as targets, where the correct class has a probability of 1 and all others 0. \n",
    "    Label smoothing softens this, assigning the correct class a probability slightly less than 1 and distributing the rest over the incorrect classes. \n",
    "    This helps avoid overfitting and makes the model less certain in its predictions, which tends to improve performance.\n",
    "    \n",
    "    size: number of classes (vocabulary size).\n",
    "    padding_idx: index of the padding token (which we should ignore)\n",
    "    smoothing: the smoothing factor (e.g., 0.1 means 90% confidence on correct class, 10% distributed among others)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # Use Kullback-Leibler Divergence as the loss, which measures how one probability distribution diverges from another\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    # x: predicted log-probabilities from the model (log_softmax should be applied before passing).\n",
    "\t# target: true class indices (as integers).\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        \n",
    "        # filling the distribution with a small probability for each class (excluding padding and correct class)\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        \n",
    "        # Overwrite the correct class with the confidence value (e.g., 0.9 if smoothing=0.1)\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "       \n",
    "        # Set padding class probability to zero\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        \n",
    "        # If any target labels are padding tokens, make their full distribution all zeros (so they’re ignored in loss calculation)\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        \n",
    "        # Store the true distribution and compute the KL-divergence loss against the prediction x\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetica Data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch_size, nbatches, device=\"cpu\"):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10)).to(device)\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm)\n",
    "        \n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, device=\"cpu\"):\n",
    "    ''' Predicts a translation using greedy decoding for simplicity '''\n",
    "    \n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).fill_(start_symbol).type_as(src.data).to(device)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat(\n",
    "            [ys, torch.zeros(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "        )\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   3.06 | Tokens / Sec:  3464.4 | Learning Rate: 5.5e-06\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   2.06 | Tokens / Sec:  8909.1 | Learning Rate: 6.1e-05\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.70 | Tokens / Sec:  8031.9 | Learning Rate: 1.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   1.41 | Tokens / Sec:  8653.8 | Learning Rate: 1.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.95 | Tokens / Sec:  8506.8 | Learning Rate: 2.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.57 | Tokens / Sec:  8817.3 | Learning Rate: 2.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.33 | Tokens / Sec:  8554.3 | Learning Rate: 3.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.28 | Tokens / Sec:  8499.6 | Learning Rate: 3.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  8629.4 | Learning Rate: 4.5e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.23 | Tokens / Sec:  8604.2 | Learning Rate: 5.0e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.20 | Tokens / Sec:  8658.2 | Learning Rate: 5.6e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec:  8674.4 | Learning Rate: 6.1e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.14 | Tokens / Sec:  8637.8 | Learning Rate: 6.7e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.16 | Tokens / Sec:  8582.4 | Learning Rate: 7.2e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:  8500.3 | Learning Rate: 7.8e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.13 | Tokens / Sec:  8546.1 | Learning Rate: 8.3e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.18 | Tokens / Sec:  8598.1 | Learning Rate: 8.9e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.11 | Tokens / Sec:  8725.4 | Learning Rate: 9.4e-04\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.20 | Tokens / Sec:  8624.6 | Learning Rate: 1.0e-03\n",
      "Epoch Step:      1 | Accumulation Step:   2 | Loss:   0.15 | Tokens / Sec:  8996.4 | Learning Rate: 1.1e-03\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "RUN_EXAMPLES = True\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "        \n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None\n",
    "        \n",
    "        \n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "         \n",
    "\n",
    "# Train the simple copy task.\n",
    "def example_simple_model():\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")  # ①\n",
    "   \n",
    "    V = 11    # number of classes\n",
    "    criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0).to(device)  # ②\n",
    "    \n",
    "    model = make_model(V, V, N=2).to(device)  # ③\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
    "    \n",
    "    # LambdaLR lets you define your own learning‐rate schedule by supplying a function that \n",
    "    # maps the current step (or epoch) → a multiplicative factor for the base LR.\n",
    "    \n",
    "    # •\tstep: the current update count (when you call scheduler.step() each minibatch or epoch).\n",
    "\t# •\tmodel_size: the model’s hidden dimension (often called d_model in transformer code). Here you grab it from your source‐embedding’s d_model attribute.\n",
    "\t# •\tfactor: an arbitrary multiplier (1.0 means “no extra scaling”).\n",
    "\t# •\twarmup: number of steps to linearly increase the LR before decaying.\n",
    "    lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: rate(step, model_size=model.src_embed[0].d_model, factor=1.0, warmup=400))\n",
    "\n",
    "    # number of sentences (or examples) per minibatch\n",
    "    batch_size = 80\n",
    "    # Training\n",
    "    for epoch in range(20):\n",
    "        model.train()  # Puts your model into “training” mode (enables dropout, layer-norm updates, etc.).\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 20, device),  # ④\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train\",\n",
    "        )\n",
    "        model.eval()    # Disables dropout and other training-only behaviors.\n",
    "        run_epoch(\n",
    "            data_gen(V, batch_size, 5, device),  # ⑤\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion),\n",
    "            # Stubs that do nothing on .step().\n",
    "            # This lets us call the same run_epoch API without accidentally updating weights or LRs.\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            # run_epoch sees this and skips backpropagation entirely (only sums/returns loss)\n",
    "            mode=\"eval\",\n",
    "        )[0]\n",
    "\n",
    "    # Inference / Greedy Decoding\n",
    "    model.eval()\n",
    "    src = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]).to(device)  # ⑥\n",
    "    max_len = src.shape[1]\n",
    "    src_mask = torch.ones(1, 1, max_len).to(device)  # ⑦\n",
    "    print(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=0, device=device))\n",
    "\n",
    "execute_example(example_simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
