{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Transformer Model:\n",
    "- Following <a href=\"https://nlp.seas.harvard.edu/2018/04/03/attention.html\" target=\"_blank\">The Annotated Transformer</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax\n",
    "import copy\n",
    "import math\n",
    "tttt. . fdfd.      ssss sssssssss\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "PRINT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "- Encoder-decoder structure works the best for translation.  \n",
    "\n",
    "- The encoder maps an \n",
    "    - input sequence of symbol representations $(x_1, ..., x_n)$ to \n",
    "    - a sequence of continuous representations $z = (z_1, ..., z_n)$.\n",
    "- Given z, the decoder\n",
    "    - generates an output sequence $(y_1, ..., y_m)$ of symbols one element a time.\n",
    "    - at each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    '''A standard Encoder-Decoder architecture. Base for this and many other models.'''\n",
    "     \n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        # python 3\n",
    "        # super().__init__()\n",
    "        # python 2\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    # what are these masks for why do we need two masks?\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # src_mask (on encoder's input) is to make sure the inputs have the same length.\n",
    "        # tgt_mask (on decoder's input) is to prevent the model to cheat by seeing the results (next word it's supposed to predict), zigzag zero padding.\n",
    "        # NOTE: here it's calling the self-defined encode and decode functions, and switch the order of the parameters.\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        if PRINT:\n",
    "            print(f'in EncoderDecoder before embedding(Encoder): src: {src.shape}; src_mask: {src_mask.shape}')\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        if PRINT:\n",
    "            print(f'in EncoderDecoder before embedding(Decoder): tgt: {tgt.shape}; tgt_mask: {tgt_mask.shape}')\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    ''' Standard linear + softmax generation step '''\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    # Produce N identical layers\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    # diagonal=0: includes the main diagonal.\n",
    "\t# diagonal=1: starts one step above the main diagonal.\n",
    "\t# diagonal=-1: starts one step below the main diagonal.\n",
    "    # torch.triu(..., diagonal=1) returns the upper triangular part\n",
    "    # [[[0, 1, 1, 1, 1],\n",
    "    #   [0, 0, 1, 1, 1],\n",
    "    #   [0, 0, 0, 1, 1],\n",
    "    #   [0, 0, 0, 0, 1],\n",
    "    #   [0, 0, 0, 0, 0]]]\n",
    "    # .type(torch.uint8) converts it to 0 (for allowed) and 1 (for masked)\n",
    "    # Later when apply mask:\n",
    "    # scores = scores.masked_fill(mask == 0, 0)   # replaces elements of the tensor where the condition is True with the given value 0.\n",
    "    # scores = scores.masked_fill(mask == 1, -inf)  # replaces elements of the tensor where the condition is True with the given value -inf.\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "\n",
    "    # Return a boolean mask tensor where each element is True if the corresponding element in subsequent_mask is 0, and False if it’s 1.\n",
    "    # tensor([[[0, 1, 1],\n",
    "    #          [0, 0, 1],\n",
    "    #          [0, 0, 0]]], dtype=torch.uint8)\n",
    "    # would return\n",
    "    # tensor([[[ True, False, False],\n",
    "    #          [ True,  True, False],\n",
    "    #          [ True,  True,  True]]])\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout = 0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        # NOTE: why 4 here?\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        if PRINT:\n",
    "            print('In MHA:')\n",
    "            for i, layer in enumerate(self.linears):\n",
    "                print(f\"Layer {i}: in_features = {layer.in_features}, out_features = {layer.out_features}\")\n",
    "            \n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = (x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        mha_return_val = self.linears[-1](x)\n",
    "        if PRINT:\n",
    "            print('MHA return val:', mha_return_val.shape)\n",
    "        return mha_return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Construct a layernorm module (See citation for details).'''\n",
    "    # TODO: what is the purpose of eps?\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # NOTE: the sublayer is a lambda wrapper on the multi-head attention from encoder & decoder\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn:MultiHeadedAttention, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer:EncoderLayer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # The encoder is composed of a stack of N identical layers.\n",
    "        # repeat the entire Encoder layer (multi-head attention, feedforward, layer_norm...) N times.\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        if PRINT:\n",
    "            print(f'in Encoder after embedding: src:{x.shape}; src_mask: {mask.shape}')\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn:MultiHeadedAttention, src_attn:MultiHeadedAttention, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer:DecoderLayer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        # repeat the entire Decoder layer (2 multi-head attention, feedforward, layer_norm...) N times.\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        if PRINT:\n",
    "            print(f'in Decoder after embedding: tgt: {x.shape}; encoder_output:{memory.shape}; encoder_output_mask: {src_mask.shape}; tgt_mask:{tgt_mask.shape}')\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if PRINT:\n",
    "            print(f'in FF: input shape: {x.shape}')\n",
    "        ff_val = self.w_2(self.dropout(self.w_1(x).relu()))\n",
    "        if PRINT:\n",
    "            print(f'in FF: output shape: {x.shape}')\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does Embedding layer do:\n",
    "\n",
    "- This embedding layer maps each token index to a d_model-dimensional vector.\n",
    "- self.lut(x) Looks up embeddings for the token indices x. Then, scales the embeddings by a constant factor $$\\sqrt{d_{model}}$$  \n",
    "- This is a normalization trick to help with training stability. The dot-product    attention has a scaling factor $$\\frac{1}{\\sqrt{d_k}}$$ so scaling the input embeddings helps balance the magnitudes.  \n",
    "\n",
    "- Without this scaling, the softmax in attention could produce very small gradients, especially at the beginning of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # This is the look up table, retrieving vectors using token IDs.\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len) -> (batch_size, seq_len, d_model)\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model= 512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src.data)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    \n",
    "    print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range (10):\n",
    "#     inference_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
    "# NOTE: using bigger training datasets.\n",
    "billsum = load_dataset(\"billsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample {'text': \"SECTION 1. FAIRNESS AND ACCURACY IN HIGH STAKES EDUCATIONAL DECISIONS \\n              FOR STUDENTS.\\n\\n    (a) Findings.--Congress makes the following findings:\\n            (1) The use of large-scale achievement tests in education \\n        has grown significantly in recent years. States and local \\n        school districts have increasingly used these tests in such \\n        contexts as raising student academic standards to make high-\\n        stakes decisions with important consequences for individual \\n        students, such as tracking (assigning students to schools, \\n        programs, or classes based on achievement level), promotion of \\n        students to the next grade, and graduation of students from \\n        secondary school.\\n            (2) The serious and often adverse consequences resulting \\n        from the sole reliance on large-scale tests have increasingly \\n        resulted in questions and significant concerns by students, \\n        parents, teachers, and school administrators about how to \\n        ensure that such tests are used appropriately and in a manner \\n        that is fair.\\n            (3) In 1997, Congress directed the National Academy of \\n        Sciences to ``conduct a study and make written recommendations \\n        on appropriate methods, practices, and safeguards to ensure \\n        that, among other things,...existing and new tests that are \\n        used to assess student performance are not used in a \\n        discriminatory manner or inappropriately for student promotion, \\n        tracking, or graduation.''.\\n            (4) In 1999, the National Academy of Sciences, through its \\n        National Research Council, completed its study and issued a \\n        report entitled ``High Stakes: Testing for Tracking, Promotion \\n        and Graduation''. Guided by principles of measurement validity, \\n        attribution of cause, and effectiveness of treatment, the \\n        National Research Council made key findings for appropriate \\n        test use in educational settings, including the following:\\n                    (A) When tests are used in ways that meet relevant \\n                psychometric, legal, and educational standards, \\n                students' scores provide important information, that \\n                combined with information from other sources, can lead \\n                to decisions that promote student learning and equality \\n                of opportunity.\\n                    (B) Tests are not perfect. Test questions are a \\n                sample of possible questions that could be asked in a \\n                given area. Moreover, a test score is not an exact \\n                measure of a student's knowledge or skills.\\n                    (C) To the extent that all students are expected to \\n                meet world-class standards, there is a need to provide \\n                world-class curricula and instruction to all students. \\n                However, in most of the Nation, much needs to be done \\n                before a world-class curriculum and world-class \\n                instruction will be in place. At present, curriculum \\n                does not usually place sufficient emphasis on student \\n                understanding and application of concepts, as opposed \\n                to memorization and skill mastery. In addition, \\n                instruction in core subjects typically has been and \\n                remains highly stratified. What teachers teach and what \\n                students learn vary widely by track, with those in \\n                lower tracks receiving far less than a world-class \\n                curriculum.\\n                    (D) It is a mistake to begin educational reform by \\n                introducing tests with high stakes for individual \\n                students. If tests are to be used for high stakes \\n                decisions about individual mastery, such use should \\n                follow implementation of changes in teaching and \\n                curriculum that ensure that students have been taught \\n                the knowledge and skills on which the students will be \\n                tested.\\n                    (E) Problems of test validity are greatest among \\n                young children, and there is a greater risk of error \\n                when such tests are employed to make high stakes \\n                decisions about children who are less than 8 years old \\n                or below grade 3, or about their schools. However, \\nwell-designed assessments may be useful in monitoring trends in the \\neducational development of populations of students who have reached age \\n5.\\n            (5) The National Research Council made the following \\n        recommendations:\\n                    (A) If parents, educators, public officials, and \\n                others who share responsibility for educational \\n                outcomes are to discharge their responsibility \\n                effectively, they should have access to information \\n                about the nature and interpretation of tests and test \\n                scores. Such information should be made available to \\n                the public and should be incorporated into teacher \\n                education and into educational programs for principals, \\n                administrators, public officials, and others.\\n                    (B) A test may appropriately be used to lead \\n                curricular reform, but it should not also be used to \\n                make high-stakes decisions about individual students \\n                until test users can show that the test measures what \\n                they have been taught.\\n                    (C) High-stakes decisions such as tracking, \\n                promotion, and graduation should not automatically be \\n                made on the basis of a single test score but should be \\n                buttressed by other relevant information about the \\n                student's knowledge and skill, such as grades, teacher \\n                recommendations, and extenuating circumstances.\\n                    (D) In general, large-scale assessments should not \\n                be used to make high-stakes decisions about students \\n                who are less than 8 years old or enrolled below grade \\n                3.\\n                    (E) High-stakes testing programs should routinely \\n                include a well-designed evaluation component. \\n                Policymakers should monitor both the intended and \\n                unintended consequences of high-stake assessments on \\n                all students and on significant subgroups of students, \\n                including minorities, English-language learners, and \\n                students with disabilities.\\n            (6) These principles and findings of the National Academy \\n        of Sciences are supported in significant measure by the \\n        Standards for Educational and Psychological Testing, adopted \\n        and approved in December of 1999, by the leading experts and \\n        professional organizations on testing, including the American \\n        Educational Research Association, American Psychological \\n        Association, and the National Council on Measurement in \\n        Education.\\n    (b) Test Performance.--If performance on a single large-scale test \\nis considered as part of any decision about the retention, graduation, \\ntracking, or within-class ability grouping of an individual student by \\na State educational agency or local educational agency that receives \\nfunds under the Elementary and Secondary Education Act of 1965, such \\ntest performance shall not be the sole criterion in such decision and \\nmay be considered in making such decision only if--\\n            (1) the test, including any cut score or performance \\n        standard set or established for use on the test, meets \\n        professional standards of validity and reliability for the \\n        purpose for which the test's results are being used;\\n            (2) the test allows its users to make score interpretations \\n        in relation to a functional performance level, as distinguished \\n        from those interpretations that are made in relation to the \\n        performance of others;\\n            (3) the test is based on State or local content and \\n        performance standards and is aligned with the curriculum and \\n        classroom instruction;\\n            (4) the test follows implementation of changes in teaching \\n        and curriculum that ensure that students have been taught the \\n        knowledge and skills on which the students will be tested;\\n            (5) multiple measures of student achievement, including \\n        grades and evaluations by teachers, are utilized to ensure that \\n        scores from the test are never the only source of information \\n        used, nor the sole criterion used, in making a high-stakes \\n        decision about an individual student;\\n            (6) students tested have been provided multiple \\n        opportunities to demonstrate proficiency in the academic \\n        subject covered by the test;\\n            (7) the test is administered in accordance with the written \\n        guidance from the test developer or publisher;\\n            (8) the State educational agency or local educational \\n        agency involved has evidence that the test is of adequate \\n        technical quality for each purpose for which the test is used;\\n            (9) the State educational agency or local educational \\n        agency provides appropriate accommodations and alternate \\n        assessments for students with disabilities that provide the \\n        students with a valid opportunity to show what the students \\n        know and can do;\\n            (10) the State educational agency or local educational \\n        agency provides appropriate accommodations and alternative \\n        assessments for students with limited English proficiency (if \\n        the agency involved determines that the students have not \\n        achieved sufficient English proficiency to ensure that the test \\n        will validly and reliably measure the subject matter knowledge \\nand skills of the students), including--\\n                    (A) the use of a test other than an English-only \\n                test;\\n                    (B) the use of alternate assessments (consisting of \\n                psychometrically equivalent tests in the students' \\n                native language) in order to provide such students with \\n                a valid and reliable opportunity to demonstrate what \\n                the students know and can do; and\\n                    (C) in a case in which the Secretary of Education \\n                determines that more than 5 percent of the students \\n                enrolled in kindergarten through grade 12 in a State \\n                are members of a single language minority group and are \\n                limited English proficient--\\n                            (i) the assessment of the students in that \\n                        group using tests developed in the language of \\n                        that group, if the State or local educational \\nagency determines that such tests are more likely than English-only \\ntests to yield accurate and reliable information regarding what those \\nstudents know and can do; or\\n                            (ii) if the language of the group is oral \\n                        or unwritten or, in the case of Alaska Natives \\n                        and other American Indians, if the predominant \\n                        language of the group is historically \\n                        unwritten, the furnishing of oral instructions, \\n                        assistance, and other necessary information to \\n                        such students relating to the English-only \\n                        test; and\\n            (11) the test is not used for a decision about promotion or \\n        placement in special education for a child below the age of 8 \\n        or third grade.\\n    (c) Evaluations.--\\n            (1) State educational agencies.--Each State educational \\n        agency that receives funds under the Elementary and Secondary \\n        Education Act of 1965 and uses a large-scale test as part of a \\n        high stakes decision described in subsection (b), shall \\n        periodically conduct a comprehensive evaluation of the impact \\n        of high stakes decisions on students' education and educational \\n        outcomes, with particular consideration given to the impact on \\n        individual students and subgroups of students disaggregated by \\n        socioeconomic status, race, ethnicity, limited English \\n        proficiency, disability, and gender. The State educational \\n        agency shall make the results of the evaluation available to \\n        the public and shall provide clear and comprehensible \\n        information about the nature, use, and interpretation of the \\n        test and the scores the test generate.\\n            (2) Local educational agency.--Each local educational \\n        agency that receives funds under the Elementary and Secondary \\n        Education Act of 1965, uses a large-scale test as part of a \\n        high stakes decision described in subsection (b), and is \\n        located in a State that does not conduct an evaluation under \\n        paragraph (1), shall periodically conduct a comprehensive \\n        evaluation of the impact of high stakes decisions on students' \\n        education and educational outcomes, with particular \\n        consideration given to the impact on individual students and \\n        subgroups of students disaggregated by socioeconomic status, \\n        race, ethnicity, limited English proficiency, disability, and \\n        gender. The local educational agency shall make the results of \\n        the evaluation available to the public and shall provide clear \\n        and comprehensible information about the nature, use, and \\n        interpretation of the test and the scores the test generate.\\n            (3) Department of education.--The Secretary shall--\\n                    (A) conduct an evaluation similar to the evaluation \\n                described in paragraph (1) among a representative \\n                sample of States and local educational agencies;\\n                    (B) report the results of such evaluation to \\n                Congress; and\\n                    (C) make the results of the evaluation available to \\n                the public.\\n    (d) Definitions.--In this section:\\n            (1) In general.--The terms used in this section have the \\n        meanings given the terms in section 14101 of the Elementary and \\n        Secondary Education Act of 1965.\\n            (2) Large-scale test.--The term ``large-scale test'' means \\n        a test that is administered and scored under conditions uniform \\n        to all students so that the test scores are comparable across \\n        individuals.\\n            (3) Sole criterion.--The term ``sole criterion'' means the \\n        only one standard (such as a test score) used to make a \\n        judgment or a decision, including a step-wise decisionmaking \\n        procedure where students must reach or exceed one criterion \\n        (such as a cut score of a test) independent of or before other \\n        criteria can be considered.\", 'summary': \"Establishes certain requirements relating to the use of large-scale standardized tests by State and local educational agencies (SEAs and LEAs) that receive funds under the Elementary and Secondary Education Act of 1965 (ESEA).Prohibits performance on a large-scale test from being the sole determinant of any decision about an individual student's retention, graduation, tracking, or within-class ability grouping. Allows such test performance to be considered in making such decision only if specified criteria are met.Requires evaluations of the impact of standardized tests used in high stakes decisions on students' education and educational outcomes, particularly on individuals and subgroups disaggregated by socioeconomic status, race, ethnicity, limited English proficiency, disability, and gender, to be carried out by: (1) SEAs receiving ESEA funds; (2) LEAs receiving ESEA funds located in States that do not do such evaluations; and (3) the Secretary of Education.\", 'title': 'To provide for fairness and accuracy in high stakes educational decisions for students.'}\n",
      "training dataset size: 15159\n",
      "testing dataset size: 3790\n"
     ]
    }
   ],
   "source": [
    "# Split data to train(0.8) & test(0.2)\n",
    "billsum_split = billsum.train_test_split(test_size=0.2)\n",
    "print('sample', billsum_split[\"train\"][0])\n",
    "print('training dataset size:', len(billsum_split['train']))\n",
    "print('testing dataset size:', len(billsum_split['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=64, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 989/989 [00:02<00:00, 444.09 examples/s]\n",
      "Map: 100%|██████████| 248/248 [00:00<00:00, 431.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_billsum = billsum_split.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'train': (989, 6), 'test': (248, 6)}\n",
      " dict_keys(['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print('', tokenized_billsum.shape)\n",
    "print('', tokenized_billsum['test'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [21603, 10, 37, 151, 13, 8, 1015, 13, 1826, 103, 3, 35, 2708, 38, 6963, 10, 180, 3073, 9562, 1300, 5568, 460, 13606, 13, 8, 9836, 3636, 19, 21012, 12, 608, 10, 460, 13606, 5, 41, 9, 61, 242, 3659, 13, 48, 1375, 6, 8, 826, 4903, 7, 1581, 10, 5637, 105, 15291, 127, 1208, 364, 6152, 153, 598, 46, 5936, 53, 1745, 24, 8201, 28, 820, 42, 722, 12, 1899, 2765, 12, 1912, 364, 21, 8, 820, 42, 722, 11, 24, 1912, 7, 66, 13, 8, 826, 3621, 10, 41, 188, 61, 31663, 6203, 28, 820, 11, 722, 21, 4573, 224, 38, 8, 97, 11, 286, 213, 8, 364, 33, 12, 36, 937, 6, 8, 686, 13, 161, 6, 8, 464, 1124, 6, 11, 8, 463, 11, 594, 13, 8, 364, 5, 41, 279, 61, 30197, 7, 14023, 42, 3, 864, 7, 6732, 4128, 13, 2765, 6, 237, 3, 99, 2765, 7365, 8, 269, 12, 9460, 806, 14023, 5, 41, 254, 61, 419, 17, 13676, 8, 5015, 12, 12317, 42, 3, 864, 7, 6732, 3, 9, 10416, 12, 430, 1188, 42, 884, 116, 8, 10416, 19, 4187, 29452, 57, 3, 9, 806, 1188, 42, 884, 5, 41, 308, 61, 282, 6732, 7, 42, 3, 864, 7, 6732, 7, 2765, 12, 1912, 364, 21, 820, 42, 722, 5, 41, 427, 61, 2821, 7, 8, 1080, 13, 726, 13, 2765, 6, 823, 42, 59, 190, 21862, 5, 41, 371, 61, 5077, 7, 2765, 45, 165, 293, 905, 42, 3744, 5, 41, 517, 61, 419, 17, 13676, 1]\n",
      " [17061, 53, 973, 2389, 2311, 24, 46, 3490, 13, 3, 9, 7234, 364, 6152, 6, 38, 4802, 6, 36, 1866, 5547, 5, 17061, 53, 973, 795, 24, 3, 9, 12374, 13, 175, 9848, 19, 24584, 179, 38, 3, 9, 1817, 1778, 15, 152, 127, 5, 100, 2876, 133, 6, 28, 824, 5763, 7, 6, 143, 8, 5547, 726, 5971, 5383, 12, 3, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "print('', tokenized_billsum['test'][0]['input_ids'])\n",
    "print('', tokenized_billsum['test'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49246\n",
      "22364\n"
     ]
    }
   ],
   "source": [
    "src_vocab = [item['text'].split() for item in tokenized_billsum['train']]\n",
    "flattened_src = set([item for sublist in src_vocab for item in sublist])\n",
    "print(len(flattened_src))\n",
    "\n",
    "target_vocab = [item['text'].split() for item in tokenized_billsum['test']]\n",
    "flattened_target = set([item for sublist in target_vocab for item in sublist])\n",
    "print(len(flattened_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarize: The people of the State of California do enact as follows: SECTION 1. Section 21107.8 of the Vehicle Code is amended to read: 21107.8. (a) (1) Any city or county may, by ordinance or resolution, find and declare that there are privately owned and maintained offstreet parking facilities as described in the ordinance or resolution within the city or county that are generally held open for use of the public for purposes of vehicular parking. Upon enactment by a city or county of the ordinance or resolution, Sections 22350, 23103, and 23109 and the provisions of Division 16.5 (commencing with Section 38000) shall apply to privately owned and maintained offstreet parking facilities, except as provided in subdivision (b). (2) (A) If a city or county enacts an ordinance or resolution authorized by paragraph (1), a city or county may include in that ordinance or resolution authorization for the operator of a privately owned and maintained offstreet parking facility to regulate unauthorized parking in that facility. (B) (i) If a city or county has exercised its authority pursuant to subparagraph (A) and unauthorized parking is regulated in</s>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_billsum['train'][0]['input_ids'])\n",
    "# tokens = tokenizer.convert_ids_to_tokens([0])\n",
    "# Join tokens without any separator\n",
    "joined_text = ''.join(tokens)\n",
    "# Replace the marker with a space and strip any leading/trailing spaces\n",
    "final_text = joined_text.replace('▁', ' ').strip()\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n",
      "989\n"
     ]
    }
   ],
   "source": [
    "# Batching the data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "input_tokens_src = [input_ids['input_ids'] for input_ids in tokenized_billsum['train']]\n",
    "input_tokens_target = [label['labels'] for label in tokenized_billsum['train']]\n",
    "print(len(input_tokens_src))\n",
    "print(len(input_tokens_target))\n",
    "\n",
    "# Convert each list into a tensor\n",
    "src_tensors = [torch.tensor(seq, dtype=torch.long) for seq in input_tokens_src]\n",
    "target_tensors = [torch.tensor(seq, dtype=torch.long) for seq in input_tokens_target]\n",
    "\n",
    "# Pad the sequences (e.g., using padding_value=0, and setting batch_first=True)\n",
    "padded_src = pad_sequence(src_tensors, batch_first=True, padding_value=0)\n",
    "padded_target = pad_sequence(target_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create a TensorDataset using the padded tensors\n",
    "train_dataset = TensorDataset(padded_src, padded_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: '<bos>', 2: '<eos>', 3: 'the', 4: 'cat', 5: 'sat', 6: 'on', 7: 'mat', 8: 'summary', 9: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary & Tokenizer\n",
    "dummy_vocab = {\n",
    "    \"<pad>\": 0,\n",
    "    \"<bos>\": 1,\n",
    "    \"<eos>\": 2,\n",
    "    \"the\": 3,\n",
    "    \"cat\": 4,\n",
    "    \"sat\": 5,\n",
    "    \"on\": 6,\n",
    "    \"mat\": 7,\n",
    "    \"summary\": 8,\n",
    "    \".\": 9\n",
    "}\n",
    "\n",
    "# tokenizer reverse\n",
    "dummy_id2word = {v: k for k, v in dummy_vocab.items()}\n",
    "print(dummy_id2word)\n",
    "\n",
    "# input and expected summary\n",
    "dummy_input_text = \"the cat sat on the mat .\"\n",
    "dummy_summary_text = \"cat on mat .\"\n",
    "\n",
    "# Tokenized\n",
    "dummy_input_ids  = torch.tensor([1, 3, 4, 5, 6, 3, 7, 9, 2])  # <bos> the cat sat on the mat . <eos>\n",
    "dummy_summary_ids = torch.tensor([1, 4, 6, 7, 9, 2])          # <bos> cat on mat . <eos>\n",
    "\n",
    "# Define masks\n",
    "dummy_src_mask = (dummy_input_ids != 0).unsqueeze(-2)\n",
    "\n",
    "# decoder receives:\n",
    "# At each decoding step i, it tries to predict tgt_output[i] from previous tgt_input[:i+1].\n",
    "# i =     0,       1,         2\n",
    "# in:    [1]    [1, 4]    [1, 4, 6]\n",
    "# out:   [4]      [6]        [7]\n",
    "dummy_tgt_input = [1, 4, 6, 7, 9]     # <bos> cat on mat .\n",
    "dummy_tgt_output = [4, 6, 7, 9, 2]    # cat on mat . <eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each step:\n",
    "•\tEmbeddings convert token IDs to vectors\n",
    "•\tSelf-attention looks at previous tokens\n",
    "•\tDecoder cross-attends to encoder output (from the full input sentence)\n",
    "•\tOutput logits are projected to vocab size and softmax gives probabilities\n",
    "•\tYou take the argmax to get the next token\n",
    "\n",
    "During decoding, the decoder looks at the encoder’s outputs (which represent the input sentence) using cross-attention.\n",
    "This is how the decoder knows what the input was, so it can generate a relevant output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_src_mask(src, pad_token=0):\n",
    "    \"\"\"\n",
    "    src: Tensor of shape (batch_size, src_len)\n",
    "    Returns: Tensor of shape (batch_size, 1, 1, src_len)\n",
    "    \n",
    "    src != pad_token\n",
    "\t•\tThis compares every token ID in the input tensor src to the padding token ID pad_token.\n",
    "\t•\tReturns a boolean tensor of the same shape as src, where:\n",
    "        •\tTrue means the token is not padding (i.e. real input),\n",
    "        •\tFalse means the token is padding.\n",
    " \n",
    "    •   Adds a new dimension at position -2 (i.e., the second-to-last axis).\n",
    "\t•\tThis reshapes the mask to fit what the attention mechanism expects.\n",
    "    \"\"\"\n",
    "    src_mask = (src != pad_token).unsqueeze(-2)\n",
    "    return src_mask  # shape: (batch_size, 1, 1, src_len)\n",
    "\n",
    "\n",
    "def make_tgt_mask(tgt, pad_token=0):\n",
    "    \"\"\"\n",
    "    tgt: Tensor of shape (batch_size, tgt_len)\n",
    "    Returns: Tensor of shape (batch_size, 1, tgt_len, tgt_len)\n",
    "    \n",
    "    torch.tril(...)\n",
    "\t•\tApplies a lower triangular mask:\n",
    "\t•\tKeeps the values on the diagonal and below it.\n",
    "\t•\tSets everything above the diagonal to zero.\n",
    "\n",
    "    Example (if tgt_len = 4):\n",
    "    torch.tril(torch.ones(4, 4)) ➝ tensor([[1, 0, 0, 0],\n",
    "                                            [1, 1, 0, 0],\n",
    "                                            [1, 1, 1, 0],\n",
    "                                            [1, 1, 1, 1]])\n",
    "    \"\"\"\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "    # Padding mask\n",
    "    tgt_pad_mask = (tgt != pad_token).unsqueeze(-2)  # (batch_size, 1, tgt_len)\n",
    "    # subsequent mask used in the decoder to prevent “cheating” during training \n",
    "    # (i.e., to ensure that each token can only attend to itself and previous tokens, not future ones).\n",
    "    # .bool() converts the matrix to boolean values: 1 -> True (can attend) 0 -> False (cannot attend)\n",
    "    tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()  # (tgt_len, tgt_len)\n",
    "    # To Combine masks their shapes must be broadcastable.\n",
    "    # \t•\ttgt_pad_mask: [batch_size, 1, tgt_len]\n",
    "    # \t•\ttgt_sub_mask: [tgt_len, tgt_len]\n",
    "    # \t•\tWhen combined with &, PyTorch broadcasts tgt_pad_mask to [batch_size, tgt_len, tgt_len]\n",
    "    # Now, the final tgt_mask says: For each query token, which keys can it attend to? Only if:\n",
    "    # \t•\tthe key token is not padding (tgt_pad_mask == True)\n",
    "    # \t•\tand it’s not in the future (tgt_sub_mask == True)\n",
    "    tgt_mask = tgt_pad_mask & tgt_sub_mask  # (batch_size, 1, tgt_len, tgt_len)\n",
    "    return tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: torch.Size([32, 256])\n",
      "tgt_input: torch.Size([32, 63])\n",
      "src_mask: torch.Size([32, 1, 256])\n",
      "tgt_mask: torch.Size([32, 63, 63])\n",
      "in EncoderDecoder before embedding(Encoder): src: torch.Size([32, 256]); src_mask: torch.Size([32, 1, 256])\n",
      "in Encoder after embedding: src:torch.Size([32, 256, 512]); src_mask: torch.Size([32, 1, 256])\n",
      "In MHA:\n",
      "Layer 0: in_features = 512, out_features = 512\n",
      "Layer 1: in_features = 512, out_features = 512\n",
      "Layer 2: in_features = 512, out_features = 512\n",
      "Layer 3: in_features = 512, out_features = 512\n",
      "MHA return val: torch.Size([32, 256, 512])\n",
      "in FF: input shape: torch.Size([32, 256, 512])\n",
      "in FF: output shape: torch.Size([32, 256, 512])\n",
      "in EncoderDecoder before embedding(Decoder): tgt: torch.Size([32, 63]); tgt_mask: torch.Size([32, 63, 63])\n",
      "in Decoder after embedding: tgt: torch.Size([32, 63, 512]); encoder_output:torch.Size([32, 256, 512]); encoder_output_mask: torch.Size([32, 1, 256]); tgt_mask:torch.Size([32, 63, 63])\n",
      "In MHA:\n",
      "Layer 0: in_features = 512, out_features = 512\n",
      "Layer 1: in_features = 512, out_features = 512\n",
      "Layer 2: in_features = 512, out_features = 512\n",
      "Layer 3: in_features = 512, out_features = 512\n",
      "MHA return val: torch.Size([32, 63, 512])\n",
      "In MHA:\n",
      "Layer 0: in_features = 512, out_features = 512\n",
      "Layer 1: in_features = 512, out_features = 512\n",
      "Layer 2: in_features = 512, out_features = 512\n",
      "Layer 3: in_features = 512, out_features = 512\n",
      "MHA return val: torch.Size([32, 63, 512])\n",
      "in FF: input shape: torch.Size([32, 63, 512])\n",
      "in FF: output shape: torch.Size([32, 63, 512])\n",
      "EncoderDecoder output shape: torch.Size([32, 63, 512])\n",
      "Transformer output shape: torch.Size([32, 63, 22364])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 0.3175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# src_vocab_size = len(flattened_src)\n",
    "# target_vocab_size = len(flattened_target)\n",
    "# NOTE: update: Use the tokenizer vocab instead\n",
    "src_vocab_size = tokenizer.vocab_size\n",
    "target_vocab_size = tokenizer.vocab_size\n",
    "# NOTE: update: increase the model layer\n",
    "model = make_model(src_vocab_size, target_vocab_size, 4).to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Loss & Optimizer\n",
    "loss_fn = nn.NLLLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in tqdm(train_loader):\n",
    "        # print('src', src.shape)\n",
    "        src_mask = make_src_mask(src, pad_token=0)\n",
    "        \n",
    "        # Shift target for decoder input/output\n",
    "        # [nlp final project]\n",
    "        # [nlp final]\n",
    "        # [final project]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_len = tgt_input.size(1)\n",
    "        \n",
    "        # Create padding mask: (batch_size, 1, 1, tgt_len)\n",
    "        pad_mask = (tgt_input != 0).unsqueeze(-2) # pad_token = 0\n",
    "        \n",
    "        # Create look-ahead mask: (1, tgt_len, tgt_len)\n",
    "        look_ahead_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "\n",
    "        # Combine: (batch_size, 1, tgt_len, tgt_len)\n",
    "        tgt_mask = pad_mask & look_ahead_mask.unsqueeze(0)\n",
    "\n",
    "        # Move all to device\n",
    "        src = src.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "        src_mask = src_mask.to(device)\n",
    "        tgt_mask = tgt_mask.to(device)\n",
    "        \n",
    "        if PRINT:\n",
    "            print(\"src:\", src.shape)\n",
    "            print(\"tgt_input:\", tgt_input.shape)\n",
    "            print(\"src_mask:\", src_mask.shape)\n",
    "            print(\"tgt_mask:\", tgt_mask.shape)\n",
    "\n",
    "        assert tgt_mask.shape[-1] == tgt_input.shape[1], \"Target mask length doesn't match input\"\n",
    "        assert src_mask.shape[-1] == src.shape[1], \"Source mask length doesn't match input\"\n",
    "        \n",
    "        out = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        if PRINT:\n",
    "            print(f'EncoderDecoder output shape: {out.shape}')\n",
    "        \n",
    "        logits = model.generator(out)\n",
    "        if PRINT:\n",
    "            print(f'Transformer output shape: {logits.shape}')\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "        loss = loss_fn(logits, tgt_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        break\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, tokenizer):\n",
    "    # Encoding source input\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # Initializing decoder input with start token\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src).to(src.device)\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        tgt_mask = subsequent_mask(ys.size(1)).to(src.device).unsqueeze(1)\n",
    "\n",
    "        # Decoding one step\n",
    "        out = model.decode(memory, src_mask, ys, tgt_mask)\n",
    "\n",
    "        # Getting log-probabilities from generator\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = torch.argmax(prob, dim=-1).unsqueeze(1)\n",
    "\n",
    "        # Appending the predicted word to the decoder input\n",
    "        ys = torch.cat([ys, next_word], dim=1)\n",
    "\n",
    "        # Optional: stop if EOS token is predicted\n",
    "        if next_word.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return ys\n",
    "\n",
    "\n",
    "def generate_summary(model, input_text, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input abstract\n",
    "    encoding = tokenizer(\"summarize: \" + input_text,\n",
    "                         return_tensors=\"pt\",\n",
    "                         max_length=512,\n",
    "                         truncation=True,\n",
    "                         padding=\"max_length\")\n",
    "\n",
    "    src = encoding[\"input_ids\"].to(next(model.parameters()).device)\n",
    "    src_mask = encoding[\"attention_mask\"].unsqueeze(0).unsqueeze(0).to(src.device)\n",
    "\n",
    "    # Decode\n",
    "    # start_symbol = tokenizer.convert_tokens_to_ids(\"summarize\")  # or use tokenizer.bos_token_id if available\n",
    "    # NOTE: start with valid beginning\n",
    "    start_symbol = tokenizer.pad_token_id if tokenizer.bos_token_id is None else tokenizer.bos_token_id\n",
    "    decoded_ids = greedy_decode(model, src, src_mask, max_len=64, start_symbol=start_symbol, tokenizer=tokenizer)\n",
    "\n",
    "    # Convert token IDs to text\n",
    "    return tokenizer.decode(decoded_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = 'Batching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches.'\n",
    "# generated_title = generate_summary(model, sample, tokenizer)\n",
    "# print(\"Generated Title:\", generated_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the entire model\n",
    "torch.save(model, 'my_transformer_model.pth')\n",
    "\n",
    "# saving the weights only\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'Batching matters a ton for speed. We want to have very evenly divided batches, with absolutely minimal padding. To do this we have to hack a bit around the default torchtext batching. This code patches their default batching to make sure we search over enough sentences to find tight batches.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model:\n",
    "model_saved = torch.load('my_transformer_model.pth', weights_only=False)\n",
    "model_saved.eval()  # Set to eval mode if using for inference\n",
    "generated_summary = generate_summary(model_saved, input_text, tokenizer)\n",
    "print(\"Generated summary:\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the weights:\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") \n",
    "src_vocab_size = tokenizer.vocab_size\n",
    "target_vocab_size = tokenizer.vocab_size\n",
    "model_saved = make_model(src_vocab_size, target_vocab_size, N=6, d_model=768, d_ff=3072, h=8, dropout=0.15).to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_saved.load_state_dict(torch.load(\"model_weights_v3.pth\", map_location=device))\n",
    "model_saved.to(device)\n",
    "model_saved.eval()\n",
    "generated_summary = generate_summary(model_saved, input_text, tokenizer)\n",
    "print(\"Generated summary:\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
